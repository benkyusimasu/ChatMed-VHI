{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5b1424f-1316-4e62-96ab-4ef47172fbb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5b1424f-1316-4e62-96ab-4ef47172fbb8",
        "outputId": "896a5280-bfc0-4a7e-b497-843f68e9032c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON文件已生成：/content/RPI_data.json\n"
          ]
        }
      ],
      "source": [
        "#Checking data and transform the excel format into json format (For the BERT models)\n",
        "import pandas as pd\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "\n",
        "def generate_random_prefix(length=16):\n",
        "\n",
        "    return ''.join(random.choices(string.ascii_lowercase + string.digits, k=length))\n",
        "\n",
        "def generate_random_id_with_prefix(index, prefix, total_length=24):\n",
        "\n",
        "    index_str = str(index)\n",
        "    padding_length = total_length - len(prefix) - len(index_str)\n",
        "    if padding_length < 0:\n",
        "        raise ValueError(\"total_length can't hold prefix and index_str, please adjust total_length or the length of prefix.\")\n",
        "    padding = ''.join(random.choices(string.ascii_lowercase + string.digits, k=padding_length))\n",
        "    return f\"{prefix}{padding}{index_str}\"\n",
        "\n",
        "def generate_answer_json_from_xlsx(file_path, output_file):\n",
        "\n",
        "    # Load excel file\n",
        "    try:\n",
        "        data = pd.read_excel(file_path, usecols=[\"Sentence\", \"Question\", \"Answer\"])\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"can not read the columns：{e}\")\n",
        "\n",
        "    # Make sure the columns exist\n",
        "    required_columns = {\"Sentence\", \"Question\", \"Answer\"}\n",
        "    if not required_columns.issubset(data.columns):\n",
        "        raise ValueError(f\"The file doesn't have necessary column：{required_columns - set(data.columns)}\")\n",
        "\n",
        "    # generate random refix\n",
        "    prefix = generate_random_prefix()\n",
        "\n",
        "    # open output file\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for idx, row in data.iterrows():\n",
        "            context = row[\"Sentence\"]\n",
        "            question = row[\"Question\"]\n",
        "            answers = [ans.strip() for ans in str(row[\"Answer\"]).split(\";\")]\n",
        "            answer_starts = [context.find(answer) for answer in answers]\n",
        "\n",
        "            #\n",
        "            if -1 in answer_starts:\n",
        "                raise ValueError(f\"The answer is not found in the context， please check the row：\\nContext: {context}\\nAnswers: {answers}\")\n",
        "\n",
        "            # Constructing JSON object\n",
        "            obj = {\n",
        "                \"id\": generate_random_id_with_prefix(idx, prefix),\n",
        "                \"question\": question,\n",
        "                \"context\": context,\n",
        "                \"answers\": {\n",
        "                    \"answer_start\": answer_starts,\n",
        "                    \"text\": answers\n",
        "                }\n",
        "            }\n",
        "\n",
        "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "    print(f\"JSON file completed：{output_file}\")\n",
        "\n",
        "#generate json file\n",
        "input_file1 = \"/content/RNA_Protein_.xlsx\"\n",
        "output_file1 = \"/content/RPI_data.json\"\n",
        "generate_answer_json_from_xlsx(input_file1, output_file1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9c1795-ebc8-4623-8043-f1bb16f3e7a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a9c1795-ebc8-4623-8043-f1bb16f3e7a6",
        "outputId": "66feb923-5489-4046-8270-b2121805893b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4262, Val: 532, Test: 534\n"
          ]
        }
      ],
      "source": [
        "#Train Dev Test\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Load every sentence of JSON\n",
        "data = []\n",
        "with open(\"/content/PPI_RPI_Table_data.json\", 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            data.append(json.loads(line))\n",
        "\n",
        "# shuffle\n",
        "random.shuffle(data)\n",
        "\n",
        "# devide\n",
        "n_total = len(data)\n",
        "n_train = int(n_total * 0.8)\n",
        "n_val = int(n_total * 0.1)\n",
        "\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:n_train + n_val]\n",
        "test_data = data[n_train + n_val:]\n",
        "\n",
        "def save_one_line_one_json(filename, dataset):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for item in dataset:\n",
        "            output_item = {\n",
        "                \"id\": item.get(\"id\", \"\"),\n",
        "                \"question\": item.get(\"question\", \"\"),\n",
        "                \"context\": item.get(\"context\", \"\"),\n",
        "                \"answers\": item.get(\"answers\", {\"answer_start\": [], \"text\": []})\n",
        "            }\n",
        "            f.write(json.dumps(output_item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "# save\n",
        "save_one_line_one_json('train.json', train_data)\n",
        "save_one_line_one_json('val.json', val_data)\n",
        "save_one_line_one_json('test.json', test_data)\n",
        "\n",
        "print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Divide the data into 10% 25% 50%\n",
        "import json\n",
        "import random\n",
        "\n",
        "# loading\n",
        "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "# shuffle\n",
        "random.shuffle(data)\n",
        "\n",
        "# calculate\n",
        "total = len(data)\n",
        "sizes = {\n",
        "    \"10%\": int(total * 0.10),\n",
        "    \"25%\": int(total * 0.25),\n",
        "    \"50%\": int(total * 0.50),\n",
        "}\n",
        "\n",
        "# 4. save the subset\n",
        "for key, size in sizes.items():\n",
        "    subset = data[:size]\n",
        "    output_path = f\"/content/train_subset_{key}.json\"\n",
        "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        for item in subset:\n",
        "            json.dump(item, f_out)\n",
        "            f_out.write(\"\\n\")\n",
        "    print(f\"✅ Saved {key} subset ({size} samples) to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4_0CuqlnY-9",
        "outputId": "afb564a5-1976-4bec-aca4-8c4cef893d78"
      },
      "id": "L4_0CuqlnY-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved 10% subset (426 samples) to /content/train_subset_10%.json\n",
            "✅ Saved 25% subset (1065 samples) to /content/train_subset_25%.json\n",
            "✅ Saved 50% subset (2131 samples) to /content/train_subset_50%.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Transform the json file into jsonl file (for the GPT models)\n",
        "import json\n",
        "\n",
        "#Read dev.json\n",
        "with open(\"/content/train_subset_50%.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "\n",
        "#Transform into OpenAI fine-tuning format\n",
        "converted = []\n",
        "for item in data:\n",
        "    context = item[\"context\"].strip()\n",
        "    question = item[\"question\"].strip()\n",
        "    answer = item[\"answers\"][\"text\"][0].strip()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a biomedical QA assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAnswer:\"},\n",
        "        {\"role\": \"assistant\", \"content\": answer}\n",
        "    ]\n",
        "    converted.append({\"messages\": messages})\n",
        "\n",
        "#Save as JSONL file\n",
        "with open(\"train50%_finetune.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for entry in converted:\n",
        "        json.dump(entry, f, ensure_ascii=False)\n",
        "        f.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "JCdiUrbuxQpc"
      },
      "id": "JCdiUrbuxQpc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53CqHXQV45uA",
      "metadata": {
        "id": "53CqHXQV45uA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "enQ2u31w-O4p",
      "metadata": {
        "id": "enQ2u31w-O4p"
      },
      "outputs": [],
      "source": [
        "#Table data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FpRYSwb19QRi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpRYSwb19QRi",
        "outputId": "263b5914-1b31-4e42-f6e8-15e8372716fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "随机合并后的 context 已保存到：/content/500_Random_Order.xlsx\n"
          ]
        }
      ],
      "source": [
        "#Random Order Without Annotation\n",
        "# Google Drive\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "input_path  = '/content/Random_Context.xlsx'\n",
        "output_path = '/content/500_Random_Order.xlsx'\n",
        "\n",
        "\n",
        "df = pd.read_excel(input_path, header=None, dtype=str).fillna('')\n",
        "\n",
        "\n",
        "def shuffle_and_join(row):\n",
        "    tokens = [t for t in row.tolist() if t.strip()!='']\n",
        "    np.random.shuffle(tokens)\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['context'] = df.apply(shuffle_and_join, axis=1)\n",
        "\n",
        "\n",
        "pd.DataFrame({'context': df['context']}) \\\n",
        "  .to_excel(output_path, index=False)\n",
        "\n",
        "print(f\"Table_data_shuffled_without_annotation：{output_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RH_jLhLSmC2F",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH_jLhLSmC2F",
        "outputId": "c3c51d75-a821-4936-c05e-cd586bca5ed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 已保存：/content/Table4_part2.csv\n"
          ]
        }
      ],
      "source": [
        "# Random Order With Annoatation\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import random\n",
        "\n",
        "\n",
        "file_path  = '/content/Table2_part2.xlsx'\n",
        "output_csv = '/content/Table4_part2.csv'\n",
        "\n",
        "xls = pd.ExcelFile(file_path)\n",
        "sheet_names = xls.sheet_names\n",
        "\n",
        "contexts = []\n",
        "\n",
        "\n",
        "for sheet in sheet_names:\n",
        "    df = pd.read_excel(file_path, sheet_name=sheet, dtype=str).fillna('')\n",
        "    headers = df.columns.tolist()\n",
        "\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        pairs = []\n",
        "        for h in headers:\n",
        "            val = row[h].strip()\n",
        "            val = re.sub(r'[\\u00A0\\u2002\\u2003\\u2009]', ' ', val)\n",
        "            val = re.sub(r'\\s+', ' ', val).strip()\n",
        "            if val and val.lower() not in ('nan', 'none'):\n",
        "                pairs.append(f\"{h}: {val}\")\n",
        "        random.shuffle(pairs)\n",
        "        contexts.append(' '.join(pairs))\n",
        "\n",
        "\n",
        "pd.DataFrame({'context': contexts}) \\\n",
        "  .to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"Table_data_shuffled_with_annotation：{output_csv}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}