# ChatMed-VHI
In this work, we introduced ChatMed-VHI, an instruction-tuned NER model based on ChatGPT-3.5 turbo, designed to extract virus-host interactions (VHIs) from full-length biomedical research articles. 
ChatMed-VHI operates on extended context windows of 2â€“6 sentences, enabling it to model short- to mid-range cross-sentence dependencies, such as resolving co-references or capturing interactions that span multiple sentences. By curating training data from both narrative (Results, Materials and Methods) and non-narrative (tables in both main and supplementary materials) sections, we enable the model to capture contextual and structured interaction information more comprehensively. This improves extraction accuracy and supports the construction of more holistic and context-aware biomedical datasets. 
Next, we systematically compared two classes of models for VHI extraction: supervised fine-tuned BERT-based models (PubMedBERT, BioLinkBERT) and instruction-tuned GPT-based models (ChatGPT-3.5). Our results demonstrated that instruction-tuned GPT-based models consistently outperform their BERT-based counterparts in low-resource settings. Specifically, ChatMed-VHI achieved an F1 score close to 90% with fewer than 500 training examples, significantly surpassing PubMedBERT under identical training conditions. These findings highlight the data-efficient, high-performance potential of instruction-tuned LLMs for biomedical named entity recognition.
In summary, we present a practical and scalable approach for extracting VHIs from diverse textual sources within biomedical literature. Our findings underscore the promise of instruction-tuned LLMs in enhancing biomedical information extraction, particularly in settings where training data is limited or distributed across complex document structures.
